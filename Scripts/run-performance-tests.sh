#!/bin/bash

# WeaveDI ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨ Ïä§ÌÅ¨Î¶ΩÌä∏
# CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú ÏûêÎèôÏúºÎ°ú Ïã§ÌñâÎê©ÎãàÎã§.

set -euo pipefail

# ÏÉâÏÉÅ Ï†ïÏùò
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Î°úÍ∑∏ Ìï®ÏàòÎì§
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ÏÑ±Îä• ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÌååÏùº
PERFORMANCE_RESULTS="performance_results.json"
PERFORMANCE_REPORT="performance_report.md"

# ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨ Ìï®Ïàò
cleanup() {
    log_info "Cleaning up temporary files..."
    rm -f test_output.log build_output.log
}

# Ïä§ÌÅ¨Î¶ΩÌä∏ Ï¢ÖÎ£å Ïãú Ï†ïÎ¶¨
trap cleanup EXIT

# Î©îÏù∏ Ìï®Ïàò
main() {
    log_info "üöÄ Starting WeaveDI Performance Benchmark"
    echo "=================================================="
    echo "Date: $(date)"
    echo "Swift Version: $(swift --version)"
    echo "Xcode Version: $(xcodebuild -version 2>/dev/null | head -1 || echo 'N/A')"
    echo "=================================================="

    # 1. ÌôòÍ≤Ω Í≤ÄÏ¶ù
    log_info "üîç Verifying environment..."
    verify_environment

    # 2. ÌîÑÎ°úÏ†ùÌä∏ ÎπåÎìú
    log_info "üèóÔ∏è Building project..."
    build_project

    # 3. ÏÑ±Îä• ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    log_info "üìä Running performance tests..."
    run_performance_tests

    # 4. Í≤∞Í≥º Î∂ÑÏÑù
    log_info "üìà Analyzing results..."
    analyze_results

    # 5. Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
    log_info "üìÑ Generating report..."
    generate_report

    log_success "‚úÖ Performance benchmark completed successfully!"
}

# ÌôòÍ≤Ω Í≤ÄÏ¶ù Ìï®Ïàò
verify_environment() {
    # Swift Î≤ÑÏ†Ñ ÌôïÏù∏
    if ! command -v swift > /dev/null 2>&1; then
        log_error "Swift not found. Please install Swift toolchain."
        exit 1
    fi

    # Package.swift ÌååÏùº ÌôïÏù∏
    if [ ! -f "Package.swift" ]; then
        log_error "Package.swift not found. Please run this script from the project root."
        exit 1
    fi

    log_success "Environment verification passed"
}

# ÌîÑÎ°úÏ†ùÌä∏ ÎπåÎìú Ìï®Ïàò
build_project() {
    log_info "Cleaning previous build..."
    swift package clean > build_output.log 2>&1

    log_info "Resolving dependencies..."
    if ! swift package resolve >> build_output.log 2>&1; then
        log_error "Failed to resolve dependencies"
        cat build_output.log
        exit 1
    fi

    log_info "Building in release mode..."
    if ! swift build --configuration release >> build_output.log 2>&1; then
        log_error "Failed to build project"
        cat build_output.log
        exit 1
    fi

    log_success "Project built successfully"
}

# ÏÑ±Îä• ÌÖåÏä§Ìä∏ Ïã§Ìñâ Ìï®Ïàò
run_performance_tests() {
    log_info "Running WeaveDI benchmarks with detailed metrics..."

    # Î≤§ÏπòÎßàÌÅ¨ Ïã§Ìñâ Ï†Ñ Î™®Îìú ÏÑ†ÌÉù
    local benchmark_mode="--performance"
    local output_format="--json"
    local results_file="benchmark_results.json"

    # CI ÌôòÍ≤ΩÏóê Îî∞Î•∏ Î™®Îìú Ï°∞Ï†ï
    if [ "${CI:-false}" = "true" ]; then
        benchmark_mode="--quick"
        log_info "CI environment detected, using quick benchmark mode"
    fi

    # Í∏∞Ï§ÄÏπò ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
    if [ -f "performance_baseline.json" ]; then
        log_info "Baseline found, will compare performance"
        baseline_arg="--baseline performance_baseline.json"
    else
        log_warning "No baseline found, current results will be used as baseline"
        baseline_arg=""
    fi

    # ÏÉàÎ°úÏö¥ Î≤§ÏπòÎßàÌÅ¨ ÏãúÏä§ÌÖú Ïã§Ìñâ
    log_info "Executing: swift run Benchmarks $benchmark_mode $output_format $results_file $baseline_arg"

    if swift run Benchmarks $benchmark_mode $output_format $results_file $baseline_arg 2>&1 | tee test_output.log; then
        log_success "WeaveDI benchmarks completed successfully"

        # Í≤∞Í≥º ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
        if [ -f "$results_file" ]; then
            log_success "Benchmark results exported to $results_file"
        fi
    else
        log_warning "Some benchmarks may have failed, but continuing with analysis"
    fi

    # Í∏∞Ï°¥ ÏÑ±Îä• ÌÖåÏä§Ìä∏ÎèÑ Ïã§Ìñâ (Ìò∏ÌôòÏÑ±ÏùÑ ÏúÑÌï¥)
    log_info "Running legacy XCTest performance tests..."
    if swift test --filter PerformanceTests 2>&1 | tee -a test_output.log; then
        log_success "Legacy performance tests completed"
    else
        log_warning "Some legacy performance tests may have failed"
    fi

    # ÌÖåÏä§Ìä∏ Í≤∞Í≥ºÏóêÏÑú ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï∂îÏ∂ú
    extract_performance_metrics
}

# ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï∂îÏ∂ú Ìï®Ïàò
extract_performance_metrics() {
    log_info "Extracting performance metrics..."

    # ÏÉàÎ°úÏö¥ Î≤§ÏπòÎßàÌÅ¨ Í≤∞Í≥ºÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ Ï≤òÎ¶¨
    if [ -f "benchmark_results.json" ]; then
        log_info "Processing new benchmark results..."
        cp "benchmark_results.json" "$PERFORMANCE_RESULTS"
        log_success "New benchmark metrics processed"
    else
        log_info "Processing legacy XCTest results..."

        # XCTest ÏÑ±Îä• Í≤∞Í≥ºÎ•º JSON ÌòïÌÉúÎ°ú Î≥ÄÌôò
        cat > "$PERFORMANCE_RESULTS" << EOF
{
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "swift_version": "$(swift --version | head -1)",
  "platform": "$(uname -m)",
  "benchmark_type": "legacy_xctest",
  "tests": [
EOF

        # ÌÖåÏä§Ìä∏ Í≤∞Í≥ºÏóêÏÑú ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÌååÏã±
        local first_test=true
        while IFS= read -r line; do
            if [[ $line =~ Test\ Case.*measured.*seconds ]]; then
                if [ "$first_test" = false ]; then
                    echo "," >> "$PERFORMANCE_RESULTS"
                fi

                # ÌÖåÏä§Ìä∏ Ïù¥Î¶ÑÍ≥º ÏãúÍ∞Ñ Ï∂îÏ∂ú
                test_name=$(echo "$line" | sed -n 's/.*Test Case.*\[\(.*\)\].*/\1/p')
                execution_time=$(echo "$line" | sed -n 's/.*measured \([0-9.]*\) seconds.*/\1/p')

                cat >> "$PERFORMANCE_RESULTS" << EOF
    {
      "test_name": "$test_name",
      "execution_time": $execution_time,
      "unit": "seconds",
      "type": "xctest_measurement"
    }
EOF
                first_test=false
            fi
        done < test_output.log

        cat >> "$PERFORMANCE_RESULTS" << EOF
  ]
}
EOF
    fi

    # Î≤§ÏπòÎßàÌÅ¨ ÏöîÏïΩ Ï†ïÎ≥¥ Ï∂îÏ∂ú
    if grep -q "üìä Total operations" test_output.log; then
        log_info "Extracting benchmark summary..."

        # ÏöîÏïΩ Ï†ïÎ≥¥Î•º Î≥ÑÎèÑ ÌååÏùºÎ°ú Ï†ÄÏû•
        cat > "benchmark_summary.txt" << EOF
$(grep -E "(üìä|‚è±Ô∏è|üöÄ)" test_output.log)
EOF

        log_success "Benchmark summary extracted"
    fi

    log_success "Performance metrics extracted to $PERFORMANCE_RESULTS"
}

# Í≤∞Í≥º Î∂ÑÏÑù Ìï®Ïàò
analyze_results() {
    log_info "Analyzing performance results..."

    # Í∏∞Ï§ÄÏπòÏôÄ ÎπÑÍµê (Í∏∞Ï§ÄÏπò ÌååÏùºÏù¥ ÏûàÎäî Í≤ΩÏö∞)
    if [ -f "performance_baseline.json" ]; then
        log_info "Comparing with baseline performance..."
        compare_with_baseline
    else
        log_warning "No baseline found. Current results will be used as baseline."
        cp "$PERFORMANCE_RESULTS" "performance_baseline.json"
    fi

    # ÏÑ±Îä• ÏûÑÍ≥ÑÍ∞í Í≤ÄÏÇ¨
    check_performance_thresholds
}

# Í∏∞Ï§ÄÏπòÏôÄ ÎπÑÍµê Ìï®Ïàò
compare_with_baseline() {
    # Í∞ÑÎã®Ìïú ÏÑ±Îä• ÎπÑÍµê (Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Îçî Ï†ïÍµêÌïú Î∂ÑÏÑù ÌïÑÏöî)
    log_info "Performance comparison will be included in the report"
}

# ÏÑ±Îä• ÏûÑÍ≥ÑÍ∞í Í≤ÄÏÇ¨ Ìï®Ïàò
check_performance_thresholds() {
    # ÏÑ±Îä• ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï (Ï¥à Îã®ÏúÑ)
    local THRESHOLD_SINGLE_DEPENDENCY=0.01
    local THRESHOLD_COMPLEX_GRAPH=0.1
    local THRESHOLD_CONCURRENT=1.0

    log_info "Checking performance thresholds..."

    # ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º Ïãú Í≤ΩÍ≥† (Ïã§Ï†úÎ°úÎäî JSON ÌååÏã± ÌïÑÏöî)
    if grep -q "testSingleDependencyResolutionPerformance" test_output.log; then
        log_info "Single dependency resolution test found"
    fi

    log_success "Performance threshold check completed"
}

# Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Ìï®Ïàò
generate_report() {
    log_info "Generating performance report..."

    cat > "$PERFORMANCE_REPORT" << EOF
# üöÄ WeaveDI Performance Benchmark Report

## üìä Test Environment
- **Date**: $(date)
- **Swift Version**: $(swift --version | head -1)
- **Platform**: $(uname -s) $(uname -m)
- **Xcode Version**: $(xcodebuild -version 2>/dev/null | head -1 || echo 'N/A')

## üìà Performance Results

### Key Metrics
EOF

    # ÏÑ±Îä• Í≤∞Í≥ºÎ•º ÎßàÌÅ¨Îã§Ïö¥ ÌëúÎ°ú Î≥ÄÌôò
    if [ -f "$PERFORMANCE_RESULTS" ]; then
        echo "" >> "$PERFORMANCE_REPORT"
        echo "| Test Name | Avg Time (ms) | Throughput (ops/sec) | Status |" >> "$PERFORMANCE_REPORT"
        echo "|-----------|---------------|---------------------|--------|" >> "$PERFORMANCE_REPORT"

        # JSON Î∞∞Ïó¥ÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (Í∞ÑÎã®Ìïú Î∞©Î≤ï)
        if grep -q "testName" "$PERFORMANCE_RESULTS"; then
            # ÏÉàÎ°úÏö¥ Î≤§ÏπòÎßàÌÅ¨ ÌòïÏãù
            python3 -c "
import json
import sys
try:
    with open('$PERFORMANCE_RESULTS', 'r') as f:
        data = json.load(f)
    if isinstance(data, list):
        for item in data:
            test_name = item.get('testName', 'Unknown')
            avg_time = item.get('averageTime', 0) * 1000  # ms Îã®ÏúÑÎ°ú Î≥ÄÌôò
            throughput = item.get('throughput', 0)
            print(f'| {test_name} | {avg_time:.4f} | {throughput:.0f} | ‚úÖ |')
except Exception as e:
    print('| Error parsing results | - | - | ‚ùå |')
" >> "$PERFORMANCE_REPORT" 2>/dev/null || {
                # PythonÏù¥ ÏóÜÎäî Í≤ΩÏö∞ fallback
                grep -o '"testName":"[^"]*"' "$PERFORMANCE_RESULTS" | while IFS= read -r line; do
                    test_name=$(echo "$line" | sed 's/"testName":"//' | sed 's/"//')
                    echo "| $test_name | - | - | ‚úÖ |" >> "$PERFORMANCE_REPORT"
                done
            }
        else
            # Î†àÍ±∞Ïãú XCTest ÌòïÏãù
            grep -o '"test_name": "[^"]*"' "$PERFORMANCE_RESULTS" | while IFS= read -r line; do
                test_name=$(echo "$line" | sed 's/"test_name": "//' | sed 's/"//')
                echo "| $test_name | - | - | ‚úÖ |" >> "$PERFORMANCE_REPORT"
            done
        fi
    fi

    cat >> "$PERFORMANCE_REPORT" << EOF

### üìã Test Summary
EOF

    # ÌÖåÏä§Ìä∏ Î°úÍ∑∏ÏóêÏÑú ÏöîÏïΩ Ï†ïÎ≥¥ Ï∂îÏ∂ú
    if [ -f "test_output.log" ]; then
        echo "" >> "$PERFORMANCE_REPORT"
        echo "\`\`\`" >> "$PERFORMANCE_REPORT"
        grep -E "(Test Suite|Test Case|measured)" test_output.log | head -20 >> "$PERFORMANCE_REPORT"
        echo "\`\`\`" >> "$PERFORMANCE_REPORT"
    fi

    cat >> "$PERFORMANCE_REPORT" << EOF

### üéØ Performance Goals
- **Single Dependency Resolution**: < 0.01 seconds
- **Complex Dependency Graph**: < 0.1 seconds
- **Concurrent Resolution**: < 1.0 seconds
- **Memory Usage**: Optimized allocation patterns

### üìù Recommendations
$(generate_recommendations)

---
*Report generated by WeaveDI Performance Benchmark Script*
*Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")*
EOF

    log_success "Performance report generated: $PERFORMANCE_REPORT"
}

# Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ± Ìï®Ïàò
generate_recommendations() {
    echo "- Monitor performance trends over time"
    echo "- Consider optimization if any test exceeds threshold by 20%"
    echo "- Update baseline after significant performance improvements"
    echo "- Run performance tests in consistent environment"
}

# ÎèÑÏõÄÎßê Ï∂úÎ†• Ìï®Ïàò
show_help() {
    cat << EOF
WeaveDI Performance Benchmark Script

Usage: $0 [OPTIONS]

Options:
    -h, --help          Show this help message
    -v, --verbose       Enable verbose output
    --clean            Clean previous results before running
    --baseline         Update performance baseline

Examples:
    $0                  Run performance tests
    $0 --verbose        Run with verbose output
    $0 --clean          Clean and run tests
    $0 --baseline       Update baseline after running tests

Files generated:
    - performance_results.json    Raw performance data
    - performance_report.md       Human-readable report
    - performance_baseline.json   Baseline for comparison
EOF
}

# Î™ÖÎ†πÌñâ Ïù∏Ïûê Ï≤òÎ¶¨
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            exit 0
            ;;
        -v|--verbose)
            set -x
            shift
            ;;
        --clean)
            log_info "Cleaning previous results..."
            rm -f performance_results.json performance_report.md test_output.log build_output.log
            shift
            ;;
        --baseline)
            UPDATE_BASELINE=true
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            show_help
            exit 1
            ;;
    esac
done

# Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
main

# Í∏∞Ï§ÄÏπò ÏóÖÎç∞Ïù¥Ìä∏ (ÏòµÏÖòÏù¥ ÏßÄÏ†ïÎêú Í≤ΩÏö∞)
if [ "${UPDATE_BASELINE:-false}" = true ]; then
    log_info "Updating performance baseline..."
    cp "$PERFORMANCE_RESULTS" "performance_baseline.json"
    log_success "Performance baseline updated"
fi

log_info "Performance benchmark completed. Check the following files:"
log_info "  - $PERFORMANCE_RESULTS (raw data)"
log_info "  - $PERFORMANCE_REPORT (report)"