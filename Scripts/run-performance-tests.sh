#!/bin/bash

# WeaveDI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
# CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ ìžë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

set -euo pipefail

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê·¸ í•¨ìˆ˜ë“¤
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼
PERFORMANCE_RESULTS="performance_results.json"
PERFORMANCE_REPORT="performance_report.md"

# ìž„ì‹œ íŒŒì¼ ì •ë¦¬ í•¨ìˆ˜
cleanup() {
    log_info "Cleaning up temporary files..."
    rm -f test_output.log build_output.log
}

# ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ì‹œ ì •ë¦¬
trap cleanup EXIT

# ë©”ì¸ í•¨ìˆ˜
main() {
    log_info "ðŸš€ Starting WeaveDI Performance Benchmark"
    echo "=================================================="
    echo "Date: $(date)"
    echo "Swift Version: $(swift --version)"
    echo "Xcode Version: $(xcodebuild -version 2>/dev/null | head -1 || echo 'N/A')"
    echo "=================================================="

    # 1. í™˜ê²½ ê²€ì¦
    log_info "ðŸ” Verifying environment..."
    verify_environment

    # 2. í”„ë¡œì íŠ¸ ë¹Œë“œ
    log_info "ðŸ—ï¸ Building project..."
    build_project

    # 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    log_info "ðŸ“Š Running performance tests..."
    run_performance_tests

    # 4. ê²°ê³¼ ë¶„ì„
    log_info "ðŸ“ˆ Analyzing results..."
    analyze_results

    # 5. ë¦¬í¬íŠ¸ ìƒì„±
    log_info "ðŸ“„ Generating report..."
    generate_report

    log_success "âœ… Performance benchmark completed successfully!"
}

# í™˜ê²½ ê²€ì¦ í•¨ìˆ˜
verify_environment() {
    # Swift ë²„ì „ í™•ì¸
    if ! command -v swift > /dev/null 2>&1; then
        log_error "Swift not found. Please install Swift toolchain."
        exit 1
    fi

    # Package.swift íŒŒì¼ í™•ì¸
    if [ ! -f "Package.swift" ]; then
        log_error "Package.swift not found. Please run this script from the project root."
        exit 1
    fi

    log_success "Environment verification passed"
}

# í”„ë¡œì íŠ¸ ë¹Œë“œ í•¨ìˆ˜
build_project() {
    log_info "Cleaning previous build..."
    swift package clean > build_output.log 2>&1

    log_info "Resolving dependencies..."
    if ! swift package resolve >> build_output.log 2>&1; then
        log_error "Failed to resolve dependencies"
        cat build_output.log
        exit 1
    fi

    log_info "Building in release mode..."
    if ! swift build --configuration release >> build_output.log 2>&1; then
        log_error "Failed to build project"
        cat build_output.log
        exit 1
    fi

    log_success "Project built successfully"
}

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
run_performance_tests() {
    log_info "Running WeaveDI benchmarks with detailed metrics..."

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì „ ëª¨ë“œ ì„ íƒ
    local benchmark_mode="--performance"
    local output_format="--json"
    local results_file="benchmark_results.json"

    # CI í™˜ê²½ì— ë”°ë¥¸ ëª¨ë“œ ì¡°ì •
    if [ "${CI:-false}" = "true" ]; then
        benchmark_mode="--quick"
        log_info "CI environment detected, using quick benchmark mode"
    fi

    # ê¸°ì¤€ì¹˜ íŒŒì¼ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸
    if [ -f "performance_baseline.json" ]; then
        log_info "Baseline found, will compare performance"
        baseline_arg="--baseline performance_baseline.json"
    else
        log_warning "No baseline found, current results will be used as baseline"
        baseline_arg=""
    fi

    # ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    log_info "Executing: swift run Benchmarks $benchmark_mode $output_format $results_file $baseline_arg"

    if swift run Benchmarks $benchmark_mode $output_format $results_file $baseline_arg 2>&1 | tee test_output.log; then
        log_success "WeaveDI benchmarks completed successfully"

        # ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if [ -f "$results_file" ]; then
            log_success "Benchmark results exported to $results_file"
        fi
    else
        log_warning "Some benchmarks may have failed, but continuing with analysis"
    fi

    # ê¸°ì¡´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë„ ì‹¤í–‰ (í˜¸í™˜ì„±ì„ ìœ„í•´)
    log_info "Running legacy XCTest performance tests..."
    if swift test --filter PerformanceTests 2>&1 | tee -a test_output.log; then
        log_success "Legacy performance tests completed"
    else
        log_warning "Some legacy performance tests may have failed"
    fi

    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    extract_performance_metrics
}

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ í•¨ìˆ˜
extract_performance_metrics() {
    log_info "Extracting performance metrics..."

    # ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬
    if [ -f "benchmark_results.json" ]; then
        log_info "Processing new benchmark results..."
        cp "benchmark_results.json" "$PERFORMANCE_RESULTS"
        log_success "New benchmark metrics processed"
    else
        log_info "Processing legacy XCTest results..."

        # XCTest ì„±ëŠ¥ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë³€í™˜
        cat > "$PERFORMANCE_RESULTS" << EOF
{
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "swift_version": "$(swift --version | head -1)",
  "platform": "$(uname -m)",
  "benchmark_type": "legacy_xctest",
  "tests": [
EOF

        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë°ì´í„° íŒŒì‹±
        local first_test=true
        while IFS= read -r line; do
            if [[ $line =~ Test\ Case.*measured.*seconds ]]; then
                if [ "$first_test" = false ]; then
                    echo "," >> "$PERFORMANCE_RESULTS"
                fi

                # í…ŒìŠ¤íŠ¸ ì´ë¦„ê³¼ ì‹œê°„ ì¶”ì¶œ
                test_name=$(echo "$line" | sed -n 's/.*Test Case.*\[\(.*\)\].*/\1/p')
                execution_time=$(echo "$line" | sed -n 's/.*measured \([0-9.]*\) seconds.*/\1/p')

                cat >> "$PERFORMANCE_RESULTS" << EOF
    {
      "test_name": "$test_name",
      "execution_time": $execution_time,
      "unit": "seconds",
      "type": "xctest_measurement"
    }
EOF
                first_test=false
            fi
        done < test_output.log

        cat >> "$PERFORMANCE_RESULTS" << EOF
  ]
}
EOF
    fi

    # ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì •ë³´ ì¶”ì¶œ
    if grep -q "ðŸ“Š Total operations" test_output.log; then
        log_info "Extracting benchmark summary..."

        # ìš”ì•½ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ìž¥
        cat > "benchmark_summary.txt" << EOF
$(grep -E "(ðŸ“Š|â±ï¸|ðŸš€)" test_output.log)
EOF

        log_success "Benchmark summary extracted"
    fi

    log_success "Performance metrics extracted to $PERFORMANCE_RESULTS"
}

# ê²°ê³¼ ë¶„ì„ í•¨ìˆ˜
analyze_results() {
    log_info "Analyzing performance results..."

    # ê¸°ì¤€ì¹˜ì™€ ë¹„êµ (ê¸°ì¤€ì¹˜ íŒŒì¼ì´ ìžˆëŠ” ê²½ìš°)
    if [ -f "performance_baseline.json" ]; then
        log_info "Comparing with baseline performance..."
        compare_with_baseline
    else
        log_warning "No baseline found. Current results will be used as baseline."
        cp "$PERFORMANCE_RESULTS" "performance_baseline.json"
    fi

    # ì„±ëŠ¥ ìž„ê³„ê°’ ê²€ì‚¬
    check_performance_thresholds
}

# ê¸°ì¤€ì¹˜ì™€ ë¹„êµ í•¨ìˆ˜
compare_with_baseline() {
    # ê°„ë‹¨í•œ ì„±ëŠ¥ ë¹„êµ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”)
    log_info "Performance comparison will be included in the report"
}

# ì„±ëŠ¥ ìž„ê³„ê°’ ê²€ì‚¬ í•¨ìˆ˜
check_performance_thresholds() {
    # ì„±ëŠ¥ ìž„ê³„ê°’ ì„¤ì • (ì´ˆ ë‹¨ìœ„)
    local THRESHOLD_SINGLE_DEPENDENCY=0.01
    local THRESHOLD_COMPLEX_GRAPH=0.1
    local THRESHOLD_CONCURRENT=1.0

    log_info "Checking performance thresholds..."

    # ìž„ê³„ê°’ ì´ˆê³¼ ì‹œ ê²½ê³  (ì‹¤ì œë¡œëŠ” JSON íŒŒì‹± í•„ìš”)
    if grep -q "testSingleDependencyResolutionPerformance" test_output.log; then
        log_info "Single dependency resolution test found"
    fi

    log_success "Performance threshold check completed"
}

# ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜
generate_report() {
    log_info "Generating performance report..."

    cat > "$PERFORMANCE_REPORT" << EOF
# ðŸš€ WeaveDI Performance Benchmark Report

## ðŸ“Š Test Environment
- **Date**: $(date)
- **Swift Version**: $(swift --version | head -1)
- **Platform**: $(uname -s) $(uname -m)
- **Xcode Version**: $(xcodebuild -version 2>/dev/null | head -1 || echo 'N/A')

## ðŸ“ˆ Performance Results

### Key Metrics
EOF

    # ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜
    if [ -f "$PERFORMANCE_RESULTS" ]; then
        echo "" >> "$PERFORMANCE_REPORT"
        echo "| Test Name | Avg Time (ms) | Throughput (ops/sec) | Status |" >> "$PERFORMANCE_REPORT"
        echo "|-----------|---------------|---------------------|--------|" >> "$PERFORMANCE_REPORT"

        # JSON ë°°ì—´ì—ì„œ ë°ì´í„° ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        if grep -q "testName" "$PERFORMANCE_RESULTS"; then
            # ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ í˜•ì‹
            python3 -c "
import json
import sys
try:
    with open('$PERFORMANCE_RESULTS', 'r') as f:
        data = json.load(f)
    if isinstance(data, list):
        for item in data:
            test_name = item.get('testName', 'Unknown')
            avg_time = item.get('averageTime', 0) * 1000  # ms ë‹¨ìœ„ë¡œ ë³€í™˜
            throughput = item.get('throughput', 0)
            print(f'| {test_name} | {avg_time:.4f} | {throughput:.0f} | âœ… |')
except Exception as e:
    print('| Error parsing results | - | - | âŒ |')
" >> "$PERFORMANCE_REPORT" 2>/dev/null || {
                # Pythonì´ ì—†ëŠ” ê²½ìš° fallback
                grep -o '"testName":"[^"]*"' "$PERFORMANCE_RESULTS" | while IFS= read -r line; do
                    test_name=$(echo "$line" | sed 's/"testName":"//' | sed 's/"//')
                    echo "| $test_name | - | - | âœ… |" >> "$PERFORMANCE_REPORT"
                done
            }
        else
            # ë ˆê±°ì‹œ XCTest í˜•ì‹
            grep -o '"test_name": "[^"]*"' "$PERFORMANCE_RESULTS" | while IFS= read -r line; do
                test_name=$(echo "$line" | sed 's/"test_name": "//' | sed 's/"//')
                echo "| $test_name | - | - | âœ… |" >> "$PERFORMANCE_REPORT"
            done
        fi
    fi

    cat >> "$PERFORMANCE_REPORT" << EOF

### ðŸ“‹ Test Summary
EOF

    # í…ŒìŠ¤íŠ¸ ë¡œê·¸ì—ì„œ ìš”ì•½ ì •ë³´ ì¶”ì¶œ
    if [ -f "test_output.log" ]; then
        echo "" >> "$PERFORMANCE_REPORT"
        echo "\`\`\`" >> "$PERFORMANCE_REPORT"
        grep -E "(Test Suite|Test Case|measured)" test_output.log | head -20 >> "$PERFORMANCE_REPORT"
        echo "\`\`\`" >> "$PERFORMANCE_REPORT"
    fi

    cat >> "$PERFORMANCE_REPORT" << EOF

### ðŸŽ¯ Performance Goals
- **Single Dependency Resolution**: < 0.01 seconds
- **Complex Dependency Graph**: < 0.1 seconds
- **Concurrent Resolution**: < 1.0 seconds
- **Memory Usage**: Optimized allocation patterns

### ðŸ“ Recommendations
$(generate_recommendations)

---
*Report generated by WeaveDI Performance Benchmark Script*
*Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")*
EOF

    log_success "Performance report generated: $PERFORMANCE_REPORT"
}

# ê¶Œìž¥ì‚¬í•­ ìƒì„± í•¨ìˆ˜
generate_recommendations() {
    echo "- Monitor performance trends over time"
    echo "- Consider optimization if any test exceeds threshold by 20%"
    echo "- Update baseline after significant performance improvements"
    echo "- Run performance tests in consistent environment"
}

# ë„ì›€ë§ ì¶œë ¥ í•¨ìˆ˜
show_help() {
    cat << EOF
WeaveDI Performance Benchmark Script

Usage: $0 [OPTIONS]

Options:
    -h, --help          Show this help message
    -v, --verbose       Enable verbose output
    --clean            Clean previous results before running
    --baseline         Update performance baseline

Examples:
    $0                  Run performance tests
    $0 --verbose        Run with verbose output
    $0 --clean          Clean and run tests
    $0 --baseline       Update baseline after running tests

Files generated:
    - performance_results.json    Raw performance data
    - performance_report.md       Human-readable report
    - performance_baseline.json   Baseline for comparison
EOF
}

# ëª…ë ¹í–‰ ì¸ìž ì²˜ë¦¬
while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            exit 0
            ;;
        -v|--verbose)
            set -x
            shift
            ;;
        --clean)
            log_info "Cleaning previous results..."
            rm -f performance_results.json performance_report.md test_output.log build_output.log
            shift
            ;;
        --baseline)
            UPDATE_BASELINE=true
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            show_help
            exit 1
            ;;
    esac
done

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main

# ê¸°ì¤€ì¹˜ ì—…ë°ì´íŠ¸ (ì˜µì…˜ì´ ì§€ì •ëœ ê²½ìš°)
if [ "${UPDATE_BASELINE:-false}" = true ]; then
    log_info "Updating performance baseline..."
    cp "$PERFORMANCE_RESULTS" "performance_baseline.json"
    log_success "Performance baseline updated"
fi

log_info "Performance benchmark completed. Check the following files:"
log_info "  - $PERFORMANCE_RESULTS (raw data)"
log_info "  - $PERFORMANCE_REPORT (report)"